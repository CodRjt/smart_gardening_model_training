{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets accelerate torch torchvision Pillow scikit-learn matplotlib evaluate\n",
    "# Note: Kaggle typically pre-installs torch/tensorflow, but explicitly installing ensures correct versions.\n",
    "# The '!' runs shell commands within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "#an image of dimension (1959,1959) would always exsist\n",
    "for i in range(11,15):\n",
    "    try:\n",
    "        im=Image.open(f\"/kaggle/input/probes-water-stress-detection/probes_water_stress_detection/train/adequate_water/p10_c1_img_000{i}.jpg\")\n",
    "        im=im.resize((224,224),box=(100,100,1500,1500))\n",
    "        print(im.format,im.size,im.mode)\n",
    "        display(im)\n",
    "    except FileNotFoundError:\n",
    "        print(\"file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "imgnew=cv2.imread(\"/kaggle/input/probes-water-stress-detection/probes_water_stress_detection/train/adequate_water/p10_c1_img_00008.jpg\")\n",
    "img_rgb=cv2.cvtColor(imgnew,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(\"without normalization\")\n",
    "img_01=cv2.normalize(imgnew,None,alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "img_rgb2=cv2.cvtColor(img_01,cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch \n",
    "import numpy as np\n",
    "img = cv2.imread(\"/kaggle/input/probes-water-stress-detection/probes_water_stress_detection/train/adequate_water/p10_c1_img_00004.jpg\")\n",
    "img_rgb=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img_rgb=cv2.resize(img_rgb,(224,224))\n",
    "imb_rgb=img_rgb.astype('float32')/255\n",
    "img_chw=np.transpose(img_rgb,(2,0,1))\n",
    "img_tensor=torch.tensor(img_rgb)\n",
    "imagenet_mean=torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "imgenet_std=torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "img_tensor=(img_tensor-imagenet_mean)/imagenet_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#using tensorFlow for preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "img=tf.io.read_file(\"/kaggle/input/probes-water-stress-detection/probes_water_stress_detection/train/adequate_water/p10_c1_img_00003.jpg\")\n",
    "img=tf.image.decode_jpeg(img,channels=3)\n",
    "img=tf.image.resize(img,[224,224])\n",
    "img=tf.cast(img,tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T14:59:41.788602Z",
     "iopub.status.busy": "2025-06-13T14:59:41.788390Z",
     "iopub.status.idle": "2025-06-13T14:59:49.250975Z",
     "shell.execute_reply": "2025-06-13T14:59:49.250039Z",
     "shell.execute_reply.started": "2025-06-13T14:59:41.788583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version : 2.18.0\n",
      "Found 17661 files belonging to 2 classes.\n",
      "Found 4417 files belonging to 2 classes.\n",
      "number of training batches in train_ds 497\n",
      "number of training batches in val_ds 139\n",
      "number of training batches in test_ds 55\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow_addons.optimizers import AdamW\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "print(\"Tensorflow version :\", tf.__version__)\n",
    "# print(\"Gpu available?\",tf.config.list_physical_devices('GPU'))\n",
    "Kaggle_dataset_path=\"/kaggle/input/probes-water-stress-detection/probes_water_stress_detection\"\n",
    "Image_size=(224,224)\n",
    "Batch_size=32\n",
    "Num_classes=2\n",
    "classes=[\"adequate_water\",\"low_water\"]\n",
    "seed=42\n",
    "Model_output_dir=\"/kaggle/working/my_plant_vit_model\"\n",
    "os.makedirs(Model_output_dir,exist_ok=True)\n",
    "Model_name=\"google/vit-base-patch16-224\"\n",
    "train_ds=tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(Kaggle_dataset_path,\"train\"),\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=classes,\n",
    "    image_size=Image_size,\n",
    "    interpolation=\"area\",\n",
    "    batch_size=Batch_size,\n",
    "    shuffle=True,\n",
    "    seed=seed\n",
    ")\n",
    "val_ds=tf.keras.utils.image_dataset_from_directory(\n",
    "    os.path.join(Kaggle_dataset_path,\"validation\"),\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    class_names=classes,\n",
    "    image_size=Image_size,\n",
    "    interpolation=\"area\",\n",
    "    batch_size=Batch_size,\n",
    "    shuffle=False,\n",
    "    seed=seed\n",
    ")\n",
    "train_batches=tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num=int(train_batches*0.1)\n",
    "test_ds=train_ds.take(num)\n",
    "train_ds=train_ds.skip(num)\n",
    "print(\"number of training batches in train_ds\",tf.data.experimental.cardinality(train_ds).numpy())\n",
    "print(\"number of training batches in val_ds\",tf.data.experimental.cardinality(val_ds).numpy())\n",
    "print(\"number of training batches in test_ds\",tf.data.experimental.cardinality(test_ds).numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:09:13.494097Z",
     "iopub.status.busy": "2025-06-13T15:09:13.493723Z",
     "iopub.status.idle": "2025-06-13T15:14:32.709810Z",
     "shell.execute_reply": "2025-06-13T15:14:32.708781Z",
     "shell.execute_reply.started": "2025-06-13T15:09:13.494069Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#testing for any leakage \n",
    "import hashlib\n",
    "def hash_tensor(tensor):\n",
    "    tensor_bytes=tf.io.serialize_tensor(tensor).numpy()\n",
    "    return hashlib.md5(tensor).hexdigest()\n",
    "train_hashes=set()\n",
    "for image,_ in train_ds:\n",
    "    train_hashes.add(hash_tensor(image))\n",
    "test_hashes=set()\n",
    "for image,_ in test_ds:\n",
    "    test_hashes.add(hash_tensor(image))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:15:11.692324Z",
     "iopub.status.busy": "2025-06-13T15:15:11.691985Z",
     "iopub.status.idle": "2025-06-13T15:15:11.697392Z",
     "shell.execute_reply": "2025-06-13T15:15:11.696274Z",
     "shell.execute_reply.started": "2025-06-13T15:15:11.692280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of intersection between train and test ds is 0\n"
     ]
    }
   ],
   "source": [
    "intersection = train_hashes & test_hashes\n",
    "print(f\"amount of intersection between train and test ds is {len(intersection)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T05:31:53.365747Z",
     "iopub.status.busy": "2025-06-13T05:31:53.365192Z",
     "iopub.status.idle": "2025-06-13T05:32:11.681345Z",
     "shell.execute_reply": "2025-06-13T05:32:11.680612Z",
     "shell.execute_reply.started": "2025-06-13T05:31:53.365722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69ee59aefb445d93c9ae20dcdaf7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13920ae934134860b16f95e69580a8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#image preprocessing\n",
    "from transformers import TFViTForImageClassification,AutoImageProcessor\n",
    "image_processor=AutoImageProcessor.from_pretrained(Model_name,use_fast=True)\n",
    "def preprocess_for_vit(image,label):\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "    image=tf.cast(image,tf.float32)\n",
    "    image=(image/255)*2-1\n",
    "    return image,label\n",
    "train_ds=train_ds.map(preprocess_for_vit,num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds=val_ds.map(preprocess_for_vit,num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds=test_ds.map(preprocess_for_vit,num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
    "print(\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-12T07:47:55.519010Z",
     "iopub.status.busy": "2025-06-12T07:47:55.518709Z",
     "iopub.status.idle": "2025-06-12T07:47:55.540080Z",
     "shell.execute_reply": "2025-06-12T07:47:55.538583Z",
     "shell.execute_reply.started": "2025-06-12T07:47:55.518986Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.13.3)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/arjit/OneDrive/Desktop/flask_pi/venv/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# resize_rescale_hf=tf.keras.Sequential([\n",
    "#     Resizing(224,224),\n",
    "#     Rescaling(1./255),\n",
    "#     Permute((3,1,2))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-12T08:15:05.449322Z",
     "iopub.status.busy": "2025-06-12T08:15:05.448954Z",
     "iopub.status.idle": "2025-06-12T08:15:08.533871Z",
     "shell.execute_reply": "2025-06-12T08:15:08.531354Z",
     "shell.execute_reply.started": "2025-06-12T08:15:05.449296Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained Vision Transformer: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TFViTModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFViTModel were not initialized from the PyTorch model and are newly initialized: ['vit.pooler.dense.weight', 'vit.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n\nA KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n\n\nCall arguments received by layer 'embeddings' (type TFViTEmbeddings):\n  • pixel_values=<KerasTensor shape=(None, 3, 224, 224), dtype=float32, sparse=False, name=keras_tensor_25>\n  • interpolate_pos_encoding=False\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/41055911.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#     block.trainable = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mmodel_encoder_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpooled_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_encoder_op\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tf_keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_tf_vit.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     ) -> tf.Tensor:\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         embeddings = self.patch_embeddings(\n\u001b[1;32m    130\u001b[0m             \u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolate_pos_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tf_utils.py\u001b[0m in \u001b[0;36mshape_list\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdynamic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/common/keras_tensor.py\u001b[0m in \u001b[0;36m__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tensor__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;34m\"A KerasTensor cannot be used as input to a TensorFlow function. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;34m\"A KerasTensor is a symbolic placeholder for a shape and dtype, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n\nA KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n\n\nCall arguments received by layer 'embeddings' (type TFViTEmbeddings):\n  • pixel_values=<KerasTensor shape=(None, 3, 224, 224), dtype=float32, sparse=False, name=keras_tensor_25>\n  • interpolate_pos_encoding=False\n  • training=False"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from transformers import ViTFeatureExtractor, TFViTModel\n",
    "\n",
    "# print(\"Loading pretrained Vision Transformer:\", Model_name)\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(Model_name)\n",
    "# id2label = {str(i): classes[i] for i in range(len(classes))}\n",
    "# label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# base_model = TFViTModel.from_pretrained(\n",
    "#     Model_name,\n",
    "#     num_labels=Num_classes,\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id,\n",
    "#     ignore_mismatched_sizes=True\n",
    "# )\n",
    "\n",
    "# inputs = tf.keras.Input(shape=(224, 224, 3), dtype=tf.float32)\n",
    "# x = tf.keras.layers.Lambda(lambda imgs: tf.transpose(imgs, [0, 3, 1, 2]))(inputs)\n",
    "\n",
    "# # Optional: freeze all but last 10 transformer blocks\n",
    "# # for block in base_model.vit.encoder.layer[:-10]:\n",
    "# #     block.trainable = False\n",
    "\n",
    "# model_embeddings=base_model.vit.embeddings(x)\n",
    "# model_encoder_op=base_model.vit.encoder(embeddings,training=True)\n",
    "# pooled_size=model_encoder_op[0]\n",
    "\n",
    "# pooled=base_model.vit.pooler(pooled_size)\n",
    "# # Build a Keras model that handles NHWC→NCHW and calls the HF model\n",
    "# # Preprocessing can be integrated here if you want:\n",
    "# # logits = base_model(pixel_values=x, training=True).logits\n",
    "\n",
    "# logits=tf.keras.layers.Dense(\n",
    "#     Num_classes,\n",
    "#     activation=None,\n",
    "#     name=\"classifier\"\n",
    "# )(pooled)\n",
    "# model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "\n",
    "\n",
    "# print(\"Model ready for training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T10:38:11.999190Z",
     "iopub.status.busy": "2025-06-13T10:38:11.998558Z",
     "iopub.status.idle": "2025-06-13T10:38:13.142577Z",
     "shell.execute_reply": "2025-06-13T10:38:13.141980Z",
     "shell.execute_reply.started": "2025-06-13T10:38:11.999145Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFViTForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing TFViTForImageClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFViTForImageClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n",
      "Some weights of TFViTForImageClassification were not initialized from the model checkpoint are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape (1000, 768) in the checkpoint and (768, 2) in the model instantiated\n",
      "- classifier.bias: found shape (1000,) in the checkpoint and (2,) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85800194 (327.30 MB)\n",
      "Trainable params: 85800194 (327.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "done\n",
      "dummy forward pass done\n",
      "Total trainable variables: 88\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "from transformers import ViTFeatureExtractor, TFViTForImageClassification\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ViTClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, num_labels, id2label=None, label2id=None,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 1) HF classification model (handles head + logits)\n",
    "        self.hf_model = TFViTForImageClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        self.model_name=model_name\n",
    "        self.num_labels=num_labels\n",
    "        self.id2label=id2label\n",
    "        self.label2id=label2id\n",
    "        # self.__setattr__('hf_model', hf_model)\n",
    "        # 2) (Optional) your own head instead of HF’s:\n",
    "        # self.classifier = tf.keras.layers.Dense(num_labels, name=\"classifier\")\n",
    "        # If you use your own head, remove the head from hf_model:\n",
    "        # self.hf_model.classifier = tf.identity  # no-op\n",
    "        self.hf_model.trainable = True\n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        \"\"\"\n",
    "        Returns the list of all trainable weights of this model and its sublayers,\n",
    "        specifically deferring to the internal Hugging Face model's trainable weights.\n",
    "        \"\"\"\n",
    "        # Ensure hf_model exists and is built before trying to access its weights\n",
    "        if hasattr(self, 'hf_model') and self.hf_model.built:\n",
    "            return self.hf_model.trainable_weights\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def non_trainable_weights(self):\n",
    "        \"\"\"\n",
    "        Returns the list of all non-trainable weights of this model and its sublayers,\n",
    "        specifically deferring to the internal Hugging Face model's non-trainable weights.\n",
    "        \"\"\"\n",
    "        # Ensure hf_model exists and is built before trying to access its weights\n",
    "        if hasattr(self, 'hf_model') and self.hf_model.built:\n",
    "            return self.hf_model.non_trainable_weights\n",
    "        return []\n",
    "\n",
    "    def call(self, images, training=False):\n",
    "        \"\"\"\n",
    "        images: a real tf.Tensor of shape (batch, 224,224,3), dtype float32\n",
    "        \"\"\"\n",
    "        # A) (Optional) normalize [0..255] → [0..1]\n",
    "        # images = tf.cast(images, tf.float32) / 255.0\n",
    "\n",
    "        # B) NHWC → NCHW for HF\n",
    "        x = tf.transpose(images, [0, 3, 1, 2])\n",
    "\n",
    "        # C) Call the HF model directly. This *won’t* be traced\n",
    "        #    during graph construction but executed at runtime.\n",
    "        outputs = self.hf_model(pixel_values=x, training=training)\n",
    "        # If you used your own head instead, do:\n",
    "        # pooled = outputs.pooler_output\n",
    "        # logits = self.classifier(pooled)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        return logits\n",
    "    def get_config(self):\n",
    "        config=super().get_config()\n",
    "        config.update({\n",
    "            \"model_name\":self.model_name,\n",
    "            \"num_labels\":self.num_labels,\n",
    "            \"id2label\":self.id2label,\n",
    "            \"label2id\":self.label2id\n",
    "        })\n",
    "        return config\n",
    "    @classmethod\n",
    "    def from_config(cls,config):\n",
    "        return cls(**config)\n",
    "        \n",
    "    def _get_regularization_losses(self):\n",
    "        # Override Keras behavior to prevent AttributeError on .regularizer\n",
    "        return []\n",
    "\n",
    "# Usage:\n",
    "\n",
    "# 1) Set up label maps if you need them for HF’s head\n",
    "id2label = {str(i): classes[i] for i in range(len(classes))}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "# 2) Instantiate\n",
    "model = ViTClassifier(\n",
    "    model_name=Model_name,\n",
    "    num_labels=Num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.hf_model.summary()\n",
    "for layer in model.hf_model.vit.encoder.layer:\n",
    "    layer.trainable = False\n",
    "for layer in model.hf_model.vit.encoder.layer[-5:]:\n",
    "    layer.trainable=True\n",
    "\n",
    "# # Make sure the head is trainable\n",
    "model.hf_model.classifier.trainable = True\n",
    "\n",
    "print(\"done\")\n",
    "_ = model(tf.random.normal([1, 224, 224, 3]))\n",
    "print(\"dummy forward pass done\")\n",
    "# Freeze entire ViT backbone\n",
    "\n",
    "# model.build(input_shape=(None, 224, 224, 3))\n",
    "# model.hf_model.summary()\n",
    "# print(\"unfreeze/freeze done\")\n",
    "# model.summary()\n",
    "print(\"Total trainable variables:\", len(model.trainable_weights))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T09:38:02.870320Z",
     "iopub.status.busy": "2025-06-12T09:38:02.869725Z",
     "iopub.status.idle": "2025-06-12T09:38:02.876851Z",
     "shell.execute_reply": "2025-06-12T09:38:02.875872Z",
     "shell.execute_reply.started": "2025-06-12T09:38:02.870296Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable variables: 200\n"
     ]
    }
   ],
   "source": [
    "# right after you freeze/unfreeze but before compile:\n",
    "# _ = model(tf.random.normal([1, 224, 224, 3]), training=False)\n",
    "print(\"Total trainable variables:\", len(model.hf_model.trainable_weights))\n",
    "for v in model.trainable_weights[:5]:\n",
    "    print(v.name, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-12T08:39:21.645244Z",
     "iopub.status.busy": "2025-06-12T08:39:21.644916Z",
     "iopub.status.idle": "2025-06-12T08:39:21.670019Z",
     "shell.execute_reply": "2025-06-12T08:39:21.669496Z",
     "shell.execute_reply.started": "2025-06-12T08:39:21.645224Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_vi_t_for_image_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vit (TFViTMainLayer)        multiple                  85798656  \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85800194 (327.30 MB)\n",
      "Trainable params: 85800194 (327.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# _ = model(tf.random.normal([1, 224, 224, 3]))\n",
    "\n",
    "# # Now summary will show the full parameter count\n",
    "# model.hf_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T18:01:31.897235Z",
     "iopub.status.busy": "2025-06-13T18:01:31.896904Z",
     "iopub.status.idle": "2025-06-13T18:01:31.902276Z",
     "shell.execute_reply": "2025-06-13T18:01:31.901307Z",
     "shell.execute_reply.started": "2025-06-13T18:01:31.897210Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#fine tuning the model\n",
    "\n",
    "# lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "#     initial_learning_rate=2e-5,\n",
    "#     decay_steps=6000, # set based on your dataset\n",
    "#     alpha=1e-6\n",
    "# )\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics=['accuracy']\n",
    "model.compile(optimizer=optimizer,loss=loss_fn,metrics=metrics)\n",
    "def scheduler(epoch,lr):\n",
    "    if epoch<7:\n",
    "        return float(lr)\n",
    "    else:\n",
    "        return float(lr*tf.math.exp(-0.1))\n",
    "\n",
    "callbacks=[\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(Model_output_dir,\"best_model_new.keras\"),\n",
    "        save_weights_only=False,\n",
    "        monitor='val_loss',\n",
    "        mode=\"min\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "                         \n",
    "    \n",
    "]\n",
    "print(\"starting model fitting\")\n",
    "history=model.fit(train_ds,\n",
    "                  validation_data=val_ds,epochs=16,callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T11:56:12.200082Z",
     "iopub.status.busy": "2025-06-13T11:56:12.199254Z",
     "iopub.status.idle": "2025-06-13T11:57:18.678687Z",
     "shell.execute_reply": "2025-06-13T11:57:18.677956Z",
     "shell.execute_reply.started": "2025-06-13T11:56:12.200055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model on test set \n",
      "best model not found at /kaggle/working/my_plant_vit_model/best_model_new.keras\n",
      "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 395ms/step - accuracy: 0.9905 - loss: 0.0305\n",
      "last trained model has the following specifications \n",
      "\n",
      "loss 0.02\n",
      "accuracy 0.99\n",
      "Classification report\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "adequate_water       1.00      0.99      0.99       881\n",
      "     low_water       0.99      1.00      0.99       879\n",
      "\n",
      "      accuracy                           0.99      1760\n",
      "     macro avg       0.99      0.99      0.99      1760\n",
      "  weighted avg       0.99      0.99      0.99      1760\n",
      "\n",
      "confusion matrix\n",
      "\n",
      "[[872   9]\n",
      " [  0 879]]\n"
     ]
    }
   ],
   "source": [
    "### test set evaluation\n",
    "# import os\n",
    "# import tensorflow as tf\n",
    "# Model_output_dir=\"/kaggle/working/my_plant_vit_model\"\n",
    "\n",
    "print(\"testing model on test set \")\n",
    "best_model_path=os.path.join(Model_output_dir,\"best_model_new.keras\")\n",
    "# if os.path.exists(best_model_path):\n",
    "#     loaded_model=tf.keras.models.load_model(\n",
    "#         best_model_path,\n",
    "#         custom_objects={\"ViTClassifier\":ViTClassifier})\n",
    "#     loss,accuracy=loaded_model.evaluate(test_ds)\n",
    "#     print(\"test loss\\n\",loss)\n",
    "#     print(\"Test Accuracy\",accuracy)\n",
    "\n",
    "#     y_true=np.concatenate([y.numpy() for x,y in test_ds],axis=0)\n",
    "#     y_pred_logits=model.predict(test_ds).logits\n",
    "#     y_pred_class=tf.argmax(y_pred_logits,axis=1).numpy()\n",
    "\n",
    "#     from sklearn.metric import classification_report,confusion_matrix\n",
    "#     print(\"Classification report\\n\")\n",
    "#     print(classification_report(y_true,y_pred_class,target_class=classes))\n",
    "#     print(\"confusion matrix\\n\")\n",
    "#     print(confusion_matrix(y_true,y_pred_class))\n",
    "# else:\n",
    "#     print(\"best model not found at\",best_model_path)\n",
    "#     loss,accuracy=model.evaluate(test_ds)\n",
    "#     print(\"last trained model has the following specifications \\n\")\n",
    "#     print(f\"loss {loss:.2f}\")\n",
    "#     print(f\"accuracy {accuracy:.2f}\")\n",
    "print(\"best model not found at\",best_model_path)\n",
    "loss,accuracy=model.evaluate(test_ds)\n",
    "print(\"last trained model has the following specifications \\n\")\n",
    "print(f\"loss {loss:.2f}\")\n",
    "print(f\"accuracy {accuracy:.2f}\")   \n",
    "y_true = []\n",
    "y_pred = []\n",
    "for x_batch, y_batch in test_ds:\n",
    "    logits = model(x_batch, training=False)\n",
    "    preds = tf.argmax(logits, axis=1)\n",
    "    y_true.extend(y_batch.numpy())\n",
    "    y_pred.extend(preds.numpy())\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(\"Classification report\\n\")\n",
    "print(classification_report(y_true,y_pred,target_names=classes))\n",
    "print(\"confusion matrix\\n\")\n",
    "print(confusion_matrix(y_true,y_pred))\n",
    "          \n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T18:00:38.059201Z",
     "iopub.status.busy": "2025-06-13T18:00:38.058788Z",
     "iopub.status.idle": "2025-06-13T18:00:38.064774Z",
     "shell.execute_reply": "2025-06-13T18:00:38.063780Z",
     "shell.execute_reply.started": "2025-06-13T18:00:38.059165Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# y_pred_logits=model.predict(test_ds)\n",
    "# y_pred_class=tf.argmax(y_pred_logits,axis=1).numpy()\n",
    "\n",
    "          \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T18:00:38.066824Z",
     "iopub.status.busy": "2025-06-13T18:00:38.066374Z",
     "iopub.status.idle": "2025-06-13T18:00:38.093018Z",
     "shell.execute_reply": "2025-06-13T18:00:38.091862Z",
     "shell.execute_reply.started": "2025-06-13T18:00:38.066785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report,confusion_matrix\n",
    "# print(\"Classification report\\n\")\n",
    "# print(classification_report(y_true,y_pred_class,target_names=classes))\n",
    "# print(\"confusion matrix\\n\")\n",
    "# print(confusion_matrix(y_true,y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T11:57:39.844464Z",
     "iopub.status.busy": "2025-06-13T11:57:39.843876Z",
     "iopub.status.idle": "2025-06-13T11:57:41.116819Z",
     "shell.execute_reply": "2025-06-13T11:57:41.116015Z",
     "shell.execute_reply.started": "2025-06-13T11:57:39.844443Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved at  /kaggle/working/my_plant_vit_model/final_saved_model.keras\n"
     ]
    }
   ],
   "source": [
    "# #saving the model explicitly\n",
    "# final_model_save_path=os.path.join(Model_output_dir,\"final_saved_model.keras\")\n",
    "# model.save(final_model_save_path,save_format=\"tf\")\n",
    "# print(\"Final model saved at \",final_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7606782,
     "sourceId": 12083617,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
